---
title: "Facebook Data Scandal: English and German Text Analysis"
author: "Tiffany Cheng"
date: "May 5, 2018"
output: html_document
out.width: 5
out.height: 4
fig.align: 'center'
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(gridExtra)
source("en and de comparisons.R")
```

## Introduction

Inspired by my study abroad adventures in Germany, I decided that I wanted to apply my English text analysis skills to the German language. From my experience, I found that Germans are very conscious of their personal data and take great precautions to keep it as secure as possible. The Facebook/Cambridge Analytica data scandal of late March to early April 2018 provided the perfect opportunity to investigate and compare reactions between English and German texts.

## Data Collection Methods

This study includes the use of Twitter tweets as well as text from news articles found online. The Twitter API was used to search for tweets both in English and in German with the following hastags: `#MarkZuckerberg`, `#Zuckerberg`, `#FacebookDataBreach`, and `#Facebook` plus the word `Zuckerberg`. There was an additional hashtag used for German, which was `#Datenskandal` or data scandal due to the small sample of German tweets. Retweets were filtered out during the search process. These tweets were used as real-time data and were collected in the time frame from April 10, 2018 to April 21, 2018. For news articles, a selection of five news companies were chosen throughout the United States and Germany to get a range of different journalistic styles and opinions. In total, five articles were chosen from each company. There was a sixth news company chosen in the United States because one company (The Housten Chronicle) only published three articles. Therefore, the Dallas News was chosen to provide the remaining two articles. The news articles serve as "historical data" and they were published between the dates of March 23, 2018 and April 11, 2018.

## Location of News Companies

The names of the six US news companies are as follows: New York Times, Chicago Tribune, Dallas News, Housten Chronicle, LA Times, and San Francisco Chronicle. Their locations on the map are seen below.

```{r}
load("us.RData")
unitedstates.map
```

The five German news companies chosen are as follows: tagesschau, Berliner Morgenpost, Deutsche Welle, Frankfurter Allgemeine Zeitung, and SÃ¼ddeutsche Zeitung. tagesshau is the most popular news source in Germany. The locations of the companies are located on the map below.

```{r}
load("germany.RData")
germany.map
```

## Word Frequency Analysis for News Articles

To get acquainted with the data, the most frequently used words in US news were counted and can be seen in the wordcloud below. As a note, words of the same color appear roughly the same amount of times and bigger words appear more often than smaller words do. From this wordcloud, it is evident that the words data, company, and user appear the most. This is followed by words related to the upcoming testimony in front of Congress (tuesday, wednesday, senate, testify, house) as well as the broader implication of the data breach (million, protect, information, personal, russian).

```{r message = FALSE, warning = FALSE}
wordcloud(news_count.en$word, news_count.en$n, min.freq = 1, max.words = 50, 
            random.order = FALSE, rot.per = 0.25, colors = brewer.pal(8, "Dark2"),
          vfont = c("sans serif", "bold"), scale=c(4,.6))
```

Next, the most frequently used words in German news were counted and reproduced below.

```{r message = FALSE, warning = FALSE}
wordcloud(news_count.de$word, news_count.de$n, min.freq = 1, max.words = 50, 
          random.order = FALSE, rot.per = 0.25, colors = brewer.pal(8, "Dark2"), scale=c(4,.5))
```


## Word Frequency Analysis for Twitter

```{r fig.cap = "Top 10 Most Commonly Used Hashtags in Tweets"}
grid.arrange(plot_tweet.hashtag_count.en, plot_tweet.hashtag_count.de, nrow = 1)
```

```{r fig.cap = "Top 10 Most Commonly Used Mentions in Tweets"}
grid.arrange(plot_tweet.mention_count.en, plot_tweet.mention_count.de, nrow = 1)
```

## Comparisons Amongst the Same Language

```{r fig.cap = "Comparing Word Usage Amongst English News Articles and Tweets", message = FALSE, warning = FALSE}
plot_frequency.en
```

Correlation from the correlation test is `r correlation.en$estimate`.

```{r fig.cap = "Comparing Word Usage Amongst German News Articles and Tweets", message = FALSE, warning = FALSE}
plot_frequency.de
```

Correlation from the correlation test is `r correlation.de$estimate`.

## Sentiment Analysis for News Articles

### For English: 

Using `nrc` sentiments lexicon for negative, then anger, then disgust.

```{r fig.cap = "`nrc` Sentiments Lexicon", message = FALSE, warning = FALSE}
wordcloud(news.negative$word, news.negative$n, max.words = 15, random.order = FALSE, 
          rot.per = 0.25, colors = brewer.pal(8, "Paired"), scale=c(4,.5))
wordcloud(news.anger$word, news.anger$n, max.words = 15, random.order = FALSE, 
          rot.per = 0.25, colors = brewer.pal(8, "Set3"), scale=c(4,.5))
wordcloud(news.disgust$word, news.disgust$n, max.words = 15, random.order = FALSE, 
          rot.per = 0.25, colors = brewer.pal(8, "Accent"), scale=c(4,.5))
```

Using `bing` sentiment lexicon results in following table and figure of top 10 most commonly used negative and positive words.

```{r fig.cap = "`bing` Sentiments Lexicon", message = FALSE, warning = FALSE}
knitr::kable(news.bing[1:3])
news.english.words %>% inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray80", "gray20"), max.words = 20, scale=c(4,.5))
```

Using `afinn` sentiment lexicon results in a sentiment score of `r news.afinn`.


### For German:

```{r fig.cap = "Postive/Negative Sentiments Lexicon", message = FALSE, warning = FALSE}
knitr::kable(news.posneg[1:3])
news.german.words %>% inner_join(posneg) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray80", "gray20"), max.words = 20, scale=c(4,.5))
```

Using sentiment assigned to each word results in a sentiment score of `r news.senval`.

## Sentiment Analysis for Twitter Tweets

### For English:

```{r fig.cap = "`nrc` Sentiments Lexicon", message = FALSE, warning = FALSE}
wordcloud(tweet.negative$word, tweet.negative$n, max.words = 15, random.order = FALSE, 
          rot.per = 0.25, colors = brewer.pal(8, "Paired"),  vfont = c("serif", "plain"), scale=c(4,.5))
wordcloud(tweet.anger$word, tweet.anger$n, max.words = 15, random.order = FALSE, 
          rot.per = 0.25, colors = brewer.pal(8, "Set2"), vfont = c("serif", "plain"), scale=c(4,.5))
wordcloud(tweet.disgust$word, tweet.disgust$n, max.words = 15, random.order = FALSE, 
          rot.per = 0.25, colors = brewer.pal(8, "Accent"), vfont = c("serif", "plain"), scale=c(4,.5))
```

`bing` sentiment lexicon and plot.

```{r fig.cap = "`nrc` Sentiments Lexicon", message = FALSE, warning = FALSE}
knitr::kable(tweet.bing[1:3])
tweet.english.words %>% inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray80", "gray20"), max.words = 20, scale=c(4,.5))
```

Using the `afinn` sentiment lexicon results in a sentiment score of `r tweet.afinn`.

### For German:

```{r fig.cap = "Postive/Negative Sentiments Lexicon", message = FALSE, warning = FALSE}
knitr::kable(news.posneg[1:3])
tweet.german.words %>% inner_join(posneg) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray80", "gray20"), max.words = 20, scale=c(4,.5))
```

Using sentiment assigned to each word results in a sentiment score of `r tweet.senval`.

## Markov Chains

### For English

Bigrams for English news articles.

```{r}
plot_news.english.bigrams
```

Bigrams for English tweets.

```{r}
plot_tweet.english.bigrams
```

### For German

Bigrams for German news articles.

```{r}
plot_news.german.bigrams
```

Bigrams for German tweets.

```{r}
plot_tweet.german.bigrams
```

